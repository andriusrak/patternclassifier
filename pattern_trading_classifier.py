# -*- coding: utf-8 -*-
"""Pattern Trading Classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fh2WjpMInF-0-CcWHL2FJK1uqHAEFu_Y
"""

#!pip install streamlit

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pickle
import os
from datetime import datetime
from sklearn.ensemble import RandomForestClassifier
import joblib
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

st.set_page_config(layout="wide", page_title="Trading Pattern Classifier")

"""App title and description"""

st.title("Stochastic Pattern Classifier & Recursive Learning System")
st.markdown("""
This app helps you classify stochastic oscillator patterns and recursively train a machine learning model.

Upload your data with OHLCV and pre-calculated stochastic oscillators
Browse through the charts and classify patterns
The model will learn from your classifications and improve over time
""")

"""Function to load data"""

@st.cache_data
def load_data(uploaded_file):
  try:
    df = pd.read_csv(uploaded_file)
    # Convert timestamp to datetime if it exists
    time_columns = ['timestamp', 'datetime', 'date', 'time']
    for col in time_columns:
      if col in df.columns:
        try:
          df[col] = pd.to_datetime(df[col])
          break
        except:
          pass
    return df
  except Exception as e:
    st.error(f"Error loading data: {e}")
    return None

def extract_stochastic_features(df, index, window_size=20, fast_col='fastk', slow_col='slowk'):
  """Extract features from a window of stochastic oscillator data"""
  if index < window_size or index + window_size >= len(df):
    return None
  # Extract window of data
  window = df.iloc[index-window_size:index+window_size].copy()

  # Basic features
  fast_line = window[fast_col].values
  slow_line = window[slow_col].values

  # Calculate differences and ratios
  diff = fast_line - slow_line

  # Identify extremes
  fast_max_idx = np.argmax(fast_line)
  fast_min_idx = np.argmin(fast_line)

  # Calculate slopes at different points
  middle_idx = len(fast_line) // 2
  pre_slope = (fast_line[middle_idx] - fast_line[0]) / middle_idx if middle_idx > 0 else 0
  post_slope = (fast_line[-1] - fast_line[middle_idx]) / (len(fast_line) - middle_idx - 1) if middle_idx < len(fast_line) - 1 else 0

  # Crossovers
  crossovers = ((fast_line[:-1] > slow_line[:-1]) & (fast_line[1:] <= slow_line[1:])) | \
              ((fast_line[:-1] < slow_line[:-1]) & (fast_line[1:] >= slow_line[1:]))
  num_crossovers = np.sum(crossovers)

  # Generate feature dictionary
  features = {
      'fast_mean': np.mean(fast_line),
      'slow_mean': np.mean(slow_line),
      'fast_std': np.std(fast_line),
      'slow_std': np.std(slow_line),
      'diff_mean': np.mean(diff),
      'diff_std': np.std(diff),
      'fast_max': np.max(fast_line),
      'fast_min': np.min(fast_line),
      'fast_max_pos': fast_max_idx / len(fast_line),
      'fast_min_pos': fast_min_idx / len(fast_line),
      'pre_slope': pre_slope,
      'post_slope': post_slope,
      'slope_change': post_slope - pre_slope,
      'num_crossovers': num_crossovers,
      'first_half_diff_mean': np.mean(diff[:len(diff)//2]),
      'second_half_diff_mean': np.mean(diff[len(diff)//2:]),
      'diff_change': np.mean(diff[len(diff)//2:]) - np.mean(diff[:len(diff)//2]),
      'fast_autocorr': np.corrcoef(fast_line[:-1], fast_line[1:])[0, 1] if len(fast_line) > 1 else 0,
  }

  # Add raw fast and slow values (downsampled)
  sample_pts = 10  # Downsample to 10 points
  indices = np.linspace(0, len(fast_line)-1, sample_pts).astype(int)
  for i, idx in enumerate(indices):
      features[f'fast_{i}'] = fast_line[idx]
      features[f'slow_{i}'] = slow_line[idx]

  return features

def features_to_dict(features_df):
  return {col: features_df[col].values for col in features_df.columns}

def plot_ohlc_with_stochastic(df, current_idx, window_size=50, fast_col='fastk', slow_col='slowk'):
  if current_idx < window_size or current_idx + window_size >= len(df):
    st.warning("Not enough data around the selected point.")
  return None
    # Extract window of data
  window = df.iloc[current_idx-window_size:current_idx+window_size].copy()

  # Create subplot with 2 rows
  fig = make_subplots(rows=2, cols=1,
                      shared_xaxes=True,
                      vertical_spacing=0.02,
                      row_heights=[0.7, 0.3])

  # Add candlestick chart
  time_col = None
  for col in ['timestamp', 'datetime', 'date', 'time']:
      if col in window.columns:
          time_col = col
          break

  x_vals = window[time_col] if time_col else window.index

  # Add candlestick trace
  fig.add_trace(go.Candlestick(
      x=x_vals,
      open=window['open'],
      high=window['high'],
      low=window['low'],
      close=window['close'],
      name='OHLC',
      increasing_line_color='green',
      decreasing_line_color='red'
  ), row=1, col=1)

  # Add stochastic oscillator traces
  fig.add_trace(go.Scatter(
      x=x_vals,
      y=window[fast_col],
      mode='lines',
      name=f'Fast Stochastic ({fast_col})',
      line=dict(color='blue')
  ), row=2, col=1)

  fig.add_trace(go.Scatter(
      x=x_vals,
      y=window[slow_col],
      mode='lines',
      name=f'Slow Stochastic ({slow_col})',
      line=dict(color='orange')
  ), row=2, col=1)

  # Add vertical line at current index
  current_time = window.iloc[window_size][time_col] if time_col else window_size

  fig.add_vline(
      x=current_time,
      line_width=2,
      line_dash="dash",
      line_color="black",
  )

  # Update layout
  fig.update_layout(
      title=f"OHLC and Stochastic Oscillator at {current_time}",
      xaxis_rangeslider_visible=False,
      height=700,
      legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="center", x=0.5),
      yaxis2=dict(range=[0, 100])  # Set stochastic range
  )

  # Update y-axis labels
  fig.update_yaxes(title_text="Price", row=1, col=1)
  fig.update_yaxes(title_text="Stochastic", row=2, col=1)

  return fig

def train_model(features_df, labels, model=None):
  if model is None:
    model = RandomForestClassifier(n_estimators=100, random_state=42)

  # Train the model
  X = features_df.drop('timestamp', axis=1) if 'timestamp' in features_df.columns else features_df
  y = labels

  try:
    model.fit(X, y)
    return model
  except Exception as e:
    st.error(f"Error training model: {e}")
    return None

def predict_pattern(model, features):
  if model is None:
    return None

  try:
      # Drop timestamp if present
      if isinstance(features, pd.DataFrame) and 'timestamp' in features.columns:
          features = features.drop('timestamp', axis=1)

      # Make prediction
      pred = model.predict([features])[0]
      prob = model.predict_proba([features])[0]

      return pred, prob
  except Exception as e:
      st.error(f"Error making prediction: {e}")
      return None, None

"""# Function to save model and training data"""

def save_model_and_data(model, features_df, labels, filename_prefix="stochastic_model"):
  try:
  # Create directory if it doesn't exist
    if not os.path.exists('./saved_models'):
      os.makedirs('./saved_models')
    # Generate timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # Save model
    model_filename = f"./saved_models/{filename_prefix}_{timestamp}.pkl"
    joblib.dump(model, model_filename)

    # Save features and labels
    data_filename = f"./saved_models/{filename_prefix}_data_{timestamp}.csv"
    data_df = features_df.copy()
    data_df['label'] = labels
    data_df.to_csv(data_filename, index=False)

    return model_filename, data_filename
  except Exception as e:
    st.error(f"Error saving model and data: {e}")
    return None, None

"""# Function to load model and training data"""

def load_model_and_data(model_file, data_file):
  try:
    # Load model
    model = joblib.load(model_file)
    # Load data
    data_df = pd.read_csv(data_file)
    features_df = data_df.drop('label', axis=1)
    labels = data_df['label']

    return model, features_df, labels
  except Exception as e:
    st.error(f"Error loading model and data: {e}")
    return None, None, None

def main():
  # Sidebar for configuration
  st.sidebar.header("Configuration")
  # File uploader
  uploaded_file = st.sidebar.file_uploader("Upload your OHLCV data with stochastic oscillators", type=["csv"])

  if uploaded_file is not None:
    # Load data
    df = load_data(uploaded_file)

    if df is not None:
        st.sidebar.success(f"Data loaded successfully! ({len(df)} rows)")

        # Stochastic column configuration
        st.sidebar.subheader("Stochastic Configuration")
        default_columns = {
            'fast': 'fastk',
            'slow': 'slowk'
        }

        # Try to auto-detect stochastic columns
        stoch_cols = [col for col in df.columns if 'k' in col.lower() or 'd' in col.lower()]

        if len(stoch_cols) >= 2:
            # Try to find fast and slow columns
            fast_candidates = [col for col in stoch_cols if 'fast' in col.lower() or 'k' in col.lower()]
            slow_candidates = [col for col in stoch_cols if 'slow' in col.lower() or 'd' in col.lower()]

            if fast_candidates:
                default_columns['fast'] = fast_candidates[0]
            if slow_candidates:
                default_columns['slow'] = slow_candidates[0]

        # Allow user to select columns
        fast_col = st.sidebar.selectbox("Fast Stochastic Column", options=df.columns, index=list(df.columns).index(default_columns['fast']) if default_columns['fast'] in df.columns else 0)
        slow_col = st.sidebar.selectbox("Slow Stochastic Column", options=df.columns, index=list(df.columns).index(default_columns['slow']) if default_columns['slow'] in df.columns else 0)

        # Model configuration
        st.sidebar.subheader("Model Configuration")
        load_existing = st.sidebar.checkbox("Load Existing Model", value=False)

        model = None
        features_df = pd.DataFrame()
        labels = []

        if load_existing:
            model_file = st.sidebar.file_uploader("Upload model file", type=["pkl", "joblib"])
            data_file = st.sidebar.file_uploader("Upload training data file", type=["csv"])

            if model_file is not None and data_file is not None:
                model, loaded_features, loaded_labels = load_model_and_data(model_file, data_file)
                if model is not None:
                    features_df = loaded_features
                    labels = loaded_labels
                    st.sidebar.success("Model and training data loaded successfully!")

        # Data navigation
        st.sidebar.subheader("Data Navigation")

        # Get timestamp column if available
        time_col = None
        for col in ['timestamp', 'datetime', 'date', 'time']:
            if col in df.columns:
                time_col = col
                break

        # Display options based on available columns
        if time_col:
            # Format the timestamps for display
            timestamp_format = st.sidebar.text_input("Timestamp Display Format", value="%Y-%m-%d %H:%M")

            try:
                timestamp_options = df[time_col].dt.strftime(timestamp_format).tolist()
                selected_timestamp = st.sidebar.selectbox("Select Timestamp", options=timestamp_options)
                current_idx = df[df[time_col].dt.strftime(timestamp_format) == selected_timestamp].index[0]
            except Exception as e:
                st.sidebar.error(f"Error formatting timestamps: {e}")
                current_idx = st.sidebar.slider("Select Data Point Index", 0, len(df)-1, len(df)//2)
        else:
            current_idx = st.sidebar.slider("Select Data Point Index", 0, len(df)-1, len(df)//2)

        # Visualization window size
        window_size = st.sidebar.slider("Window Size (bars before/after current point)", 10, 100, 50)

        # Main content
        col1, col2 = st.columns([3, 1])

        with col1:
            # Display OHLCV with stochastic
            fig = plot_ohlc_with_stochastic(df, current_idx, window_size, fast_col, slow_col)
            if fig:
                st.plotly_chart(fig, use_container_width=True)

        with col2:
            # Pattern classification
            st.subheader("Pattern Classification")

            # Extract features for the current pattern
            features = extract_stochastic_features(df, current_idx, window_size, fast_col, slow_col)

            if features:
                # Show timestamp if available
                if time_col:
                    current_time = df.iloc[current_idx][time_col]
                    st.write(f"Current time: {current_time}")

                    # Add timestamp to features for reference
                    features['timestamp'] = current_time

                # Make prediction if model exists
                if model is not None:
                    pred, prob = predict_pattern(model, features)

                    if pred is not None:
                        st.write(f"**Model prediction:** {pred}")

                        # Display probabilities
                        st.write("Prediction probabilities:")
                        proba_df = pd.DataFrame({
                            'Class': model.classes_,
                            'Probability': prob
                        })
                        st.dataframe(proba_df.sort_values('Probability', ascending=False))

                # Classification options
                classification_options = ["good_pattern", "not_pattern", "unsure"]
                pattern_label = st.radio("Classify this pattern:", options=classification_options)

                if st.button("Save Classification"):
                    # Convert features to DataFrame if it's a dict
                    if isinstance(features, dict):
                        features_row = pd.DataFrame([features])
                    else:
                        features_row = features

                    # Add to features DataFrame and labels list
                    if features_df.empty:
                        features_df = features_row
                    else:
                        features_df = pd.concat([features_df, features_row], ignore_index=True)

                    labels.append(pattern_label)

                    st.success(f"Pattern classified as '{pattern_label}' and saved!")

                    # Retrain model with new data point (recursive learning)
                    if len(set(labels)) > 1:  # Need at least 2 classes to train
                        model = train_model(features_df, labels, model)
                        st.success("Model updated with new data point!")
            else:
                st.warning("Could not extract features for the current pattern.")

        # Model training and evaluation section
        st.subheader("Model Training and Management")

        col3, col4 = st.columns(2)

        with col3:
            # Display training data
            if not features_df.empty and labels:
                st.write(f"Current training dataset: {len(labels)} examples")

                # Display class distribution
                class_counts = pd.Series(labels).value_counts()
                st.write("Class distribution:")
                st.bar_chart(class_counts)

                # Save model and data button
                if st.button("Save Model and Training Data"):
                    model_file, data_file = save_model_and_data(model, features_df, labels)
                    if model_file and data_file:
                        st.success(f"Model and data saved successfully!\n\nModel: {model_file}\nData: {data_file}")
            else:
                st.write("No training data available yet. Classify some patterns to build the dataset.")

        with col4:
            # Model evaluation
            if model is not None and len(set(labels)) > 1:
                st.write("Model Evaluation")

                # Only evaluate if we have enough data
                if len(labels) >= 10:
                    # Use time series cross-validation
                    tscv = TimeSeriesSplit(n_splits=min(5, len(labels)//5))
                    cv_scores = []

                    X = features_df.drop('timestamp', axis=1) if 'timestamp' in features_df.columns else features_df
                    y = np.array(labels)

                    try:
                        for train_idx, test_idx in tscv.split(X):
                            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
                            y_train, y_test = y[train_idx], y[test_idx]

                            # Train model on training split
                            split_model = RandomForestClassifier(n_estimators=100, random_state=42)
                            split_model.fit(X_train, y_train)

                            # Predict on test split
                            y_pred = split_model.predict(X_test)

                            # Calculate accuracy
                            acc = accuracy_score(y_test, y_pred)
                            cv_scores.append(acc)

                        # Display cross-validation results
                        st.write(f"Cross-validation accuracy: {np.mean(cv_scores):.2f} Â± {np.std(cv_scores):.2f}")

                        # Feature importance
                        if hasattr(model, 'feature_importances_'):
                            st.write("Top 10 important features:")
                            importances = pd.DataFrame({
                                'Feature': X.columns,
                                'Importance': model.feature_importances_
                            }).sort_values('Importance', ascending=False).head(10)

                            st.bar_chart(importances.set_index('Feature'))
                    except Exception as e:
                        st.error(f"Error during model evaluation: {e}")
                else:
                    st.write("Need at least 10 examples for model evaluation.")
            else:
                st.write("No model available for evaluation yet.")

        # Pattern finder section
        st.subheader("Pattern Finder")

        if model is not None and not features_df.empty:
            # Allow searching for similar patterns
            st.write("Find patterns similar to the current selection:")

            search_method = st.radio("Search method:", ["Find most confident predictions", "Find patterns similar to current"])

            if search_method == "Find most confident predictions":
                # Number of patterns to find
                n_patterns = st.slider("Number of patterns to find", 5, 50, 10)

                if st.button("Find Top Patterns"):
                    with st.spinner("Searching for patterns..."):
                        # Create progress bar
                        progress_bar = st.progress(0)

                        # Process data in chunks to improve performance
                        chunk_size = 1000
                        n_chunks = (len(df) - 2*window_size) // chunk_size + 1

                        all_predictions = []

                        for i in range(n_chunks):
                            start_idx = i * chunk_size + window_size
                            end_idx = min(start_idx + chunk_size, len(df) - window_size)

                            if start_idx >= end_idx:
                                break

                            # Update progress
                            progress_bar.progress(i / n_chunks)

                            # Extract features for this chunk
                            for idx in range(start_idx, end_idx):
                                features = extract_stochastic_features(df, idx, window_size, fast_col, slow_col)
                                if features:
                                    # Add index for reference
                                    features_with_index = features.copy()
                                    features_with_index['idx'] = idx

                                    # Make prediction
                                    try:
                                        pred, prob = predict_pattern(model, features)
                                        if pred == "good_pattern":  # Only look for good patterns
                                            all_predictions.append({
                                                'idx': idx,
                                                'pred': pred,
                                                'confidence': np.max(prob),
                                                'timestamp': df.iloc[idx][time_col] if time_col else idx
                                            })
                                    except Exception as e:
                                        continue

                        # Complete progress bar
                        progress_bar.progress(1.0)

                        # Sort by confidence
                        all_predictions.sort(key=lambda x: x['confidence'], reverse=True)

                        # Display top patterns
                        top_patterns = all_predictions[:n_patterns]

                        if top_patterns:
                            st.success(f"Found {len(top_patterns)} patterns!")

                            # Display results in a table
                            results_df = pd.DataFrame(top_patterns)
                            st.dataframe(results_df)

                            # Allow user to view a specific pattern
                            selected_pattern_idx = st.selectbox("Select pattern to view:",
                                                             options=range(len(top_patterns)),
                                                             format_func=lambda i: f"Pattern {i+1} - {top_patterns[i]['timestamp']} (Confidence: {top_patterns[i]['confidence']:.2f})")

                            # Display the selected pattern
                            pattern_idx = top_patterns[selected_pattern_idx]['idx']
                            st.plotly_chart(plot_ohlc_with_stochastic(df, pattern_idx, window_size, fast_col, slow_col), use_container_width=True)
                        else:
                            st.warning("No patterns found.")

            elif search_method == "Find patterns similar to current":
                # Extract features for current pattern
                current_features = extract_stochastic_features(df, current_idx, window_size, fast_col, slow_col)

                if current_features:
                    # Number of patterns to find
                    n_patterns = st.slider("Number of similar patterns to find", 5, 50, 10)

                    if st.button("Find Similar Patterns"):
                        with st.spinner("Searching for similar patterns..."):
                            # Create progress bar
                            progress_bar = st.progress(0)

                            # Process data in chunks
                            chunk_size = 1000
                            n_chunks = (len(df) - 2*window_size) // chunk_size + 1

                            all_similarities = []

                            # Drop timestamp if present
                            if isinstance(current_features, dict) and 'timestamp' in current_features:
                                current_features_array = np.array([v for k, v in current_features.items() if k != 'timestamp'])
                            else:
                                current_features_array = np.array(list(current_features.values()))

                            for i in range(n_chunks):
                                start_idx = i * chunk_size + window_size
                                end_idx = min(start_idx + chunk_size, len(df) - window_size)

                                if start_idx >= end_idx:
                                    break

                                # Update progress
                                progress_bar.progress(i / n_chunks)

                                # Extract features for this chunk
                                for idx in range(start_idx, end_idx):
                                    if idx == current_idx:
                                        continue  # Skip current pattern

                                    features = extract_stochastic_features(df, idx, window_size, fast_col, slow_col)
                                    if features:
                                        # Calculate similarity (Euclidean distance)
                                        try:
                                            # Drop timestamp if present
                                            if isinstance(features, dict) and 'timestamp' in features:
                                                features_array = np.array([v for k, v in features.items() if k != 'timestamp'])
                                            else:
                                                features_array = np.array(list(features.values()))

                                            # Calculate distance
                                            distance = np.linalg.norm(features_array - current_features_array)

                                            # Add to list
                                            all_similarities.append({
                                                'idx': idx,
                                                'distance': distance,
                                                'timestamp': df.iloc[idx][time_col] if time_col else idx
                                            })
                                        except Exception as e:
                                            continue

                            # Complete progress bar
                            progress_bar.progress(1.0)

                            # Sort by similarity (lowest distance)
                            all_similarities.sort(key=lambda x: x['distance'])

                            # Display top patterns
                            similar_patterns = all_similarities[:n_patterns]

                            if similar_patterns:
                                st.success(f"Found {len(similar_patterns)} similar patterns!")

                                # Display results in a table
                                results_df = pd.DataFrame(similar_patterns)
                                st.dataframe(results_df)

                                # Allow user to view a specific pattern
                                selected_pattern_idx = st.selectbox("Select pattern to view:",
                                                                 options=range(len(similar_patterns)),
                                                                 format_func=lambda i: f"Pattern {i+1} - {similar_patterns[i]['timestamp']} (Distance: {similar_patterns[i]['distance']:.2f})")

                                # Display the selected pattern
                                pattern_idx = similar_patterns[selected_pattern_idx]['idx']
                                st.plotly_chart(plot_ohlc_with_stochastic(df, pattern_idx, window_size, fast_col, slow_col), use_container_width=True)
                            else:
                                st.warning("No similar patterns found.")
                else:
                    st.warning("Could not extract features for the current pattern.")
        else:
            st.write("Train a model first to use the pattern finder.")

if __name__ == "__main__":
    main()
